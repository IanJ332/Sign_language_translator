{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "id": "jYKyWZGTVLpM",
        "outputId": "090c86ad-bb5b-48e6-9c90-50118e5f9b26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Step 1: Setting up environment and preparing raw data ---\n",
            "Mounted at /content/drive\n",
            "\n",
            "Copying and extracting all video datasets to local disk...\n",
            "Processing train_rgb_front_clips.zip...\n",
            "Processing val_rgb_front_clips.zip...\n",
            "Processing test_rgb_front_clips.zip...\n",
            "All video data is ready on the local disk.\n",
            "\n",
            "--- Step 2: Preparing labels and constants ---\n",
            "Total unique classes found across ALL datasets: 33483\n",
            "\n",
            "--- Step 4: Creating Data Generators ---\n",
            "Train and Validation Generators are ready.\n",
            "\n",
            "--- Step 5: Training the Final Optimized Model ---\n",
            "Using best hyperparameters found by Optuna: {'learning_rate': 0.00021001054934091255, 'lstm_units': 93, 'dropout_rate': 0.45079469677480605}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-6-160037408.py:94: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights='imagenet')\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_3              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">511,128</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">93</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33483</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,147,402</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m1280\u001b[0m) │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_3              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1280\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1280\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m)             │       \u001b[38;5;34m511,128\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m93\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33483\u001b[0m)          │     \u001b[38;5;34m3,147,402\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,916,514</span> (22.57 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,916,514\u001b[0m (22.57 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,658,530</span> (13.96 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,658,530\u001b[0m (13.96 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 1: val_accuracy improved from -inf to 0.00000, saving model to /content/drive/MyDrive/train-CNN+LSTM+BO/cnn_lstm_optimized_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6037s\u001b[0m 6s/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 2/50\n",
            "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 2: val_accuracy did not improve from 0.00000\n",
            "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5794s\u001b[0m 6s/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 3/50\n",
            "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 3: val_accuracy did not improve from 0.00000\n",
            "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5738s\u001b[0m 6s/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 4/50\n",
            "\u001b[1m850/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11:13\u001b[0m 6s/step - accuracy: 0.0000e+00 - loss: 0.0000e+00"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# Final Project Script: Train, Evaluate, and Summarize All Models (with Debugging)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 0. 导入所有必需的库 ---\n",
        "# (省略... 与上一版本完全相同)\n",
        "from google.colab import drive\n",
        "import cv2; import numpy as np; import os; import pandas as pd; import math; import shutil\n",
        "from tensorflow.keras.utils import Sequence, to_categorical; from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.applications import MobileNetV2; from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, TimeDistributed, LSTM, Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam; from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# --- 1. 环境设置：挂载Drive并准备原始数据 ---\n",
        "print(\"--- Step 1: Setting up environment and preparing raw data ---\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "base_drive_path = '/content/drive/MyDrive/train-CNN+LSTM+BO'; train_zip_gdrive_path = os.path.join(base_drive_path, 'train_rgb_front_clips.zip'); val_zip_gdrive_path = os.path.join(base_drive_path, 'val_rgb_front_clips.zip'); test_zip_gdrive_path = os.path.join(base_drive_path, 'test_rgb_front_clips.zip')\n",
        "train_zip_local_path = 'train_rgb_front_clips.zip'; val_zip_local_path = 'val_rgb_front_clips.zip'; test_zip_local_path = 'test_rgb_front_clips.zip'\n",
        "train_extract_folder = 'frontal_train_videos'; val_extract_folder = 'frontal_val_videos'; test_extract_folder = 'frontal_test_videos'\n",
        "print(\"\\nCopying and extracting all video datasets to local disk...\")\n",
        "for gdrive_path, local_path, extract_folder in [(train_zip_gdrive_path, train_zip_local_path, train_extract_folder), (val_zip_gdrive_path, val_zip_local_path, val_extract_folder), (test_zip_gdrive_path, test_zip_local_path, test_extract_folder)]:\n",
        "    print(f\"Processing {os.path.basename(gdrive_path)}...\"); shutil.copy(gdrive_path, local_path); os.makedirs(extract_folder, exist_ok=True)\n",
        "    with zipfile.ZipFile(local_path, 'r') as zip_ref: zip_ref.extractall(extract_folder)\n",
        "    os.remove(local_path)\n",
        "print(\"All video data is ready on the local disk.\")\n",
        "\n",
        "# --- 2. 准备标签和常量 ---\n",
        "print(\"\\n--- Step 2: Preparing labels and constants ---\")\n",
        "TRAIN_LABELS_CSV_GDRIVE = os.path.join(base_drive_path, 'how2sign_realigned_train.csv'); VAL_LABELS_CSV_GDRIVE = os.path.join(base_drive_path, 'how2sign_realigned_val.csv'); TEST_LABELS_CSV_GDRIVE = os.path.join(base_drive_path, 'how2sign_realigned_test.csv')\n",
        "shutil.copy(TRAIN_LABELS_CSV_GDRIVE, 'how2sign_realigned_train.csv'); shutil.copy(VAL_LABELS_CSV_GDRIVE, 'how2sign_realigned_val.csv'); shutil.copy(TEST_LABELS_CSV_GDRIVE, 'how2sign_realigned_test.csv')\n",
        "IMG_SIZE = 64; MAX_FRAMES = 30; BATCH_SIZE = 32\n",
        "TRAIN_VIDEO_FOLDER = 'frontal_train_videos/raw_videos'; VAL_VIDEO_FOLDER = 'frontal_val_videos/raw_videos'; TEST_VIDEO_FOLDER = 'frontal_test_videos/raw_videos'\n",
        "train_labels_df = pd.read_csv('how2sign_realigned_train.csv', sep='\\t'); val_labels_df = pd.read_csv('how2sign_realigned_val.csv', sep='\\t'); test_labels_df = pd.read_csv('how2sign_realigned_test.csv', sep='\\t')\n",
        "all_labels_df = pd.concat([train_labels_df, val_labels_df, test_labels_df], ignore_index=True)\n",
        "label_encoder = LabelEncoder(); label_encoder.fit(all_labels_df['SENTENCE']); NUM_CLASSES = len(label_encoder.classes_)\n",
        "print(f\"Total unique classes found across ALL datasets: {NUM_CLASSES}\")\n",
        "\n",
        "# --- 3. 定义数据生成器 ---\n",
        "class SignLanguageGenerator(Sequence):\n",
        "    def __init__(self, data_folder, labels_df, label_encoder, batch_size, num_classes):\n",
        "        self.data_folder = data_folder; self.labels_df = labels_df.copy(); self.label_encoder = label_encoder; self.batch_size = batch_size; self.num_classes = num_classes\n",
        "        all_disk_files = {os.path.splitext(f)[0] for f in os.listdir(self.data_folder) if f.endswith('.mp4')}\n",
        "        all_csv_files = set(self.labels_df['SENTENCE_NAME'].tolist()); valid_files = list(all_disk_files.intersection(all_csv_files))\n",
        "        self.video_files = valid_files; self.labels_df = self.labels_df[self.labels_df['SENTENCE_NAME'].isin(self.video_files)]\n",
        "    def __len__(self): return math.floor(len(self.video_files) / self.batch_size)\n",
        "    def __getitem__(self, idx):\n",
        "        batch_files = self.video_files[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_labels_df = self.labels_df[self.labels_df['SENTENCE_NAME'].isin(batch_files)]\n",
        "\n",
        "        # --- NEW DEBUGGING CODE ---\n",
        "        if len(batch_files) > 0 and len(batch_labels_df) == 0:\n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "            print(f\"CRITICAL WARNING: Data Mismatch Detected in Batch Index {idx}\")\n",
        "            print(f\"Generator sliced {len(batch_files)} filenames, but found 0 of them in the provided DataFrame.\")\n",
        "            print(\"This should not happen with the current logic. Printing slice for inspection:\")\n",
        "            print(batch_files)\n",
        "            print(\"=\"*80 + \"\\n\")\n",
        "        # --- END DEBUGGING CODE ---\n",
        "\n",
        "        X = np.zeros((len(batch_files), MAX_FRAMES, IMG_SIZE, IMG_SIZE, 3), dtype=np.float32); y_text = []\n",
        "        for i, row in enumerate(batch_labels_df.itertuples()):\n",
        "            video_path = os.path.join(self.data_folder, row.SENTENCE_NAME + '.mp4'); cap = cv2.VideoCapture(video_path); frames = []\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret: break; resized_frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE)); frames.append(resized_frame)\n",
        "            cap.release(); frames = np.array(frames)\n",
        "            if frames.size == 0: continue\n",
        "            if len(frames) > MAX_FRAMES: frames = frames[:MAX_FRAMES]\n",
        "            elif len(frames) < MAX_FRAMES:\n",
        "                pad_width = ((0, MAX_FRAMES - len(frames)), (0, 0), (0, 0), (0, 0)); frames = np.pad(frames, pad_width, mode='constant', constant_values=0)\n",
        "            X[i,] = frames / 255.0; y_text.append(row.SENTENCE)\n",
        "        try:\n",
        "            if not y_text: # 如果y_text是空的，直接返回空的数组避免transform报错\n",
        "                return np.zeros_like(X), np.zeros((X.shape[0], self.num_classes))\n",
        "            y_int = self.label_encoder.transform(y_text); y = to_categorical(y_int, num_classes=self.num_classes)\n",
        "        except ValueError: return np.zeros_like(X), np.zeros((X.shape[0], self.num_classes))\n",
        "        return X, y\n",
        "\n",
        "# --- (后面的代码保持不变) ---\n",
        "\n",
        "# --- 4. 实例化训练和验证生成器 ---\n",
        "print(\"\\n--- Step 4: Creating Data Generators ---\")\n",
        "train_generator = SignLanguageGenerator(data_folder=TRAIN_VIDEO_FOLDER, labels_df=train_labels_df, label_encoder=label_encoder, batch_size=BATCH_SIZE, num_classes=NUM_CLASSES)\n",
        "validation_generator = SignLanguageGenerator(data_folder=VAL_VIDEO_FOLDER, labels_df=val_labels_df, label_encoder=label_encoder, batch_size=BATCH_SIZE, num_classes=NUM_CLASSES)\n",
        "print(\"Train and Validation Generators are ready.\")\n",
        "\n",
        "# --- 5. 训练最终的优化模型 ---\n",
        "print(\"\\n--- Step 5: Training the Final Optimized Model ---\")\n",
        "best_params = {'learning_rate': 0.00021001054934091255, 'lstm_units': 93, 'dropout_rate': 0.45079469677480605}\n",
        "print(\"Using best hyperparameters found by Optuna:\", best_params)\n",
        "\n",
        "input_shape = (MAX_FRAMES, IMG_SIZE, IMG_SIZE, 3)\n",
        "base_model = MobileNetV2(input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights='imagenet')\n",
        "base_model.trainable = False\n",
        "video_input = Input(shape=input_shape)\n",
        "cnn_features = TimeDistributed(base_model)(video_input); cnn_features = TimeDistributed(GlobalAveragePooling2D())(cnn_features); cnn_features = Dropout(best_params['dropout_rate'])(cnn_features)\n",
        "lstm_output = LSTM(best_params['lstm_units'])(cnn_features); lstm_output = Dropout(best_params['dropout_rate'])(lstm_output)\n",
        "output_layer = Dense(NUM_CLASSES, activation='softmax')(lstm_output)\n",
        "final_model = Model(inputs=video_input, outputs=output_layer)\n",
        "\n",
        "final_model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "final_model.summary()\n",
        "\n",
        "final_model_path = '/content/drive/MyDrive/train-CNN+LSTM+BO/cnn_lstm_optimized_model.h5'\n",
        "model_checkpoint = ModelCheckpoint(filepath=final_model_path, save_best_only=True, monitor='val_accuracy', mode='max', verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "\n",
        "history_final = final_model.fit(train_generator, validation_data=validation_generator, epochs=50, callbacks=[model_checkpoint, early_stopping])\n",
        "print(\"Final Model Training Complete.\")\n",
        "\n",
        "# --- 6. 评估最终模型并生成总结 ---\n",
        "print(\"\\n--- Step 6: Evaluating the Final Model and Generating Summary ---\")\n",
        "# 加载已保存的最佳模型\n",
        "print(\"Loading the best saved optimized model for final evaluation...\")\n",
        "optimized_model = load_model(final_model_path)\n",
        "\n",
        "# 创建测试集生成器\n",
        "known_classes = set(label_encoder.classes_)\n",
        "test_labels_df_filtered = test_labels_df[test_labels_df['SENTENCE'].isin(known_classes)]\n",
        "test_generator = SignLanguageGenerator(data_folder=TEST_VIDEO_FOLDER, labels_df=test_labels_df_filtered, label_encoder=label_encoder, batch_size=BATCH_SIZE, num_classes=NUM_CLASSES)\n",
        "\n",
        "# 在测试集上评估\n",
        "print(\"\\nEvaluating on the official TEST set...\")\n",
        "y_pred_one_hot = optimized_model.predict(test_generator)\n",
        "y_pred_labels = np.argmax(y_pred_one_hot, axis=1)\n",
        "\n",
        "batches_to_run = len(test_generator)\n",
        "samples_to_consider = batches_to_run * BATCH_SIZE\n",
        "y_true_text = test_generator.labels_df['SENTENCE'].iloc[:samples_to_consider]\n",
        "y_true_int = label_encoder.transform(y_true_text)\n",
        "y_pred_labels = y_pred_labels[:len(y_true_int)]\n",
        "\n",
        "final_accuracy = accuracy_score(y_true_int, y_pred_labels)\n",
        "report = classification_report(y_true_int, y_pred_labels, output_dict=True, zero_division=0)\n",
        "final_f1_score = report['macro avg']['f1-score']\n",
        "\n",
        "# --- 7. 生成最终总结表 ---\n",
        "lstm_baseline_accuracy = 0.0017; lstm_baseline_f1 = 0.0 # 假设值\n",
        "manual_cnn_lstm_accuracy = 0.0012; manual_cnn_lstm_f1 = 0.0 # 假设值\n",
        "\n",
        "summary_data = {\n",
        "    'Model': ['1. Baseline LSTM (Keypoints)', '2. Manually-Tuned CNN-LSTM (RGB)', '3. Optimized CNN-LSTM (RGB)'],\n",
        "    'Test Accuracy': [f\"{lstm_baseline_accuracy*100:.4f}%\", f\"{manual_cnn_lstm_accuracy*100:.4f}%\", f\"{final_accuracy*100:.4f}%\"],\n",
        "    'Test F1-Score (Macro Avg)': [f\"{lstm_baseline_f1:.4f}\", f\"{manual_cnn_lstm_f1:.4f}\", f\"{final_f1_score:.4f}\"]\n",
        "}\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(\"\\n\\n--- PROJECT SUMMARY TABLE ---\")\n",
        "print(summary_df.to_markdown(index=False))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}