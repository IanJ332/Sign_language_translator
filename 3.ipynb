{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJJ7bierIGZc",
        "outputId": "b63136a8-6705-4513-eedb-2ace0eecba41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n",
            "\n",
            "Extracting train videos to local disk (this will take time but speed up training later)...\n",
            "Train videos extracted locally.\n",
            "\n",
            "Extracting validation videos to local disk...\n",
            "Validation videos extracted locally.\n",
            "\n",
            "--- Data Preparation for Phase 3 Complete ---\n"
          ]
        }
      ],
      "source": [
        "# --- 0. 导入库 ---\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# --- 1. 挂载Drive ---\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted successfully.\")\n",
        "\n",
        "# --- 2. 定义路径 ---\n",
        "base_drive_path = '/content/drive/MyDrive/train-CNN+LSTM+BO'\n",
        "train_zip_gdrive_path = os.path.join(base_drive_path, 'train_rgb_front_clips.zip')\n",
        "val_zip_gdrive_path = os.path.join(base_drive_path, 'val_rgb_front_clips.zip')\n",
        "\n",
        "# Colab本地路径\n",
        "train_extract_folder = 'frontal_train_videos'\n",
        "val_extract_folder = 'frontal_val_videos'\n",
        "\n",
        "# --- 3. 在本地解压训练集 ---\n",
        "print(\"\\nExtracting train videos to local disk (this will take time but speed up training later)...\")\n",
        "os.makedirs(train_extract_folder, exist_ok=True)\n",
        "with zipfile.ZipFile(train_zip_gdrive_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(train_extract_folder)\n",
        "print(\"Train videos extracted locally.\")\n",
        "\n",
        "# --- 4. 在本地解压验证集 ---\n",
        "print(\"\\nExtracting validation videos to local disk...\")\n",
        "os.makedirs(val_extract_folder, exist_ok=True)\n",
        "with zipfile.ZipFile(val_zip_gdrive_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(val_extract_folder)\n",
        "print(\"Validation videos extracted locally.\")\n",
        "\n",
        "print(\"\\n--- Data Preparation for Phase 3 Complete ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (此单元格顶部的导入和标签加载部分保持不变)\n",
        "# ...\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import cv2\n",
        "import optuna\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, TimeDistributed, LSTM, Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "base_drive_path = '/content/drive/MyDrive/train-CNN+LSTM+BO'\n",
        "TRAIN_LABELS_CSV = os.path.join(base_drive_path, 'how2sign_realigned_train.csv')\n",
        "VAL_LABELS_CSV = os.path.join(base_drive_path, 'how2sign_realigned_val.csv')\n",
        "train_labels_df = pd.read_csv(TRAIN_LABELS_CSV, sep='\\t')\n",
        "val_labels_df = pd.read_csv(VAL_LABELS_CSV, sep='\\t')\n",
        "all_labels_df = pd.concat([train_labels_df, val_labels_df], ignore_index=True)\n",
        "label_encoder = LabelEncoder(); label_encoder.fit(all_labels_df['SENTENCE'])\n",
        "NUM_CLASSES = len(label_encoder.classes_)\n",
        "IMG_SIZE = 64; MAX_FRAMES = 30; BATCH_SIZE = 32\n",
        "TRAIN_VIDEO_FOLDER = 'frontal_train_videos/raw_videos'\n",
        "VAL_VIDEO_FOLDER = 'frontal_val_videos/raw_videos'\n",
        "# ...\n",
        "\n",
        "# --- 2. 定义数据生成器 (最终修复版) ---\n",
        "class SignLanguageGenerator(Sequence):\n",
        "    def __init__(self, data_folder, labels_df, label_encoder, batch_size, num_classes):\n",
        "        self.data_folder = data_folder; self.labels_df = labels_df.copy(); self.label_encoder = label_encoder; self.batch_size = batch_size; self.num_classes = num_classes\n",
        "        all_disk_files = {os.path.splitext(f)[0] for f in os.listdir(self.data_folder) if f.endswith('.mp4')}\n",
        "        all_csv_files = set(self.labels_df['SENTENCE_NAME'].tolist()); valid_files = list(all_disk_files.intersection(all_csv_files))\n",
        "        self.video_files = valid_files; self.labels_df = self.labels_df[self.labels_df['SENTENCE_NAME'].isin(self.video_files)]\n",
        "        print(f\"Generator for '{self.data_folder}' initialized with {len(self.video_files)} valid files.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.floor(len(self.video_files) / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # --- THIS IS THE FINAL FIX ---\n",
        "        batch_files = self.video_files[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "        # 我们根据实际的batch_files来过滤DataFrame，确保数量一致\n",
        "        batch_labels_df = self.labels_df[self.labels_df['SENTENCE_NAME'].isin(batch_files)]\n",
        "\n",
        "        # X数组的大小应该基于过滤后的DataFrame的行数，即实际要处理的视频数\n",
        "        X = np.zeros((len(batch_labels_df), MAX_FRAMES, IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
        "        y_text = []\n",
        "\n",
        "        for i, row in enumerate(batch_labels_df.itertuples()):\n",
        "            video_path = os.path.join(self.data_folder, row.SENTENCE_NAME + '.mp4')\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "            frames = []\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret: break\n",
        "                resized_frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
        "                frames.append(resized_frame)\n",
        "            cap.release()\n",
        "            frames = np.array(frames)\n",
        "\n",
        "            if frames.size == 0:\n",
        "                # 如果视频无法读取，跳过此样本，但我们需要确保X和y的长度仍然匹配\n",
        "                # 一个简单的方法是创建一个占位的空帧数组，但这可能会影响训练\n",
        "                # 更稳健的方法是在__init__阶段就检查视频是否可读，但会非常慢\n",
        "                # 这里我们先跳过，但在循环外需要重新调整X的大小\n",
        "                continue\n",
        "\n",
        "            if len(frames) > MAX_FRAMES:\n",
        "                frames = frames[:MAX_FRAMES]\n",
        "            elif len(frames) < MAX_FRAMES:\n",
        "                pad_width = ((0, MAX_FRAMES - len(frames)), (0, 0), (0, 0), (0, 0))\n",
        "                frames = np.pad(frames, pad_width, mode='constant', constant_values=0)\n",
        "\n",
        "            X[i,] = frames / 255.0\n",
        "            y_text.append(row.SENTENCE)\n",
        "\n",
        "        # 确保X的大小和y_text的长度一致\n",
        "        if X.shape[0] != len(y_text):\n",
        "            X = X[:len(y_text)]\n",
        "\n",
        "        y_int = self.label_encoder.transform(y_text)\n",
        "        y = to_categorical(y_int, num_classes=self.num_classes)\n",
        "        return X, y\n",
        "        # --- END OF FINAL FIX ---\n",
        "\n",
        "# (实例化生成器和定义objective函数的代码保持不变)\n",
        "# ...\n",
        "train_generator = SignLanguageGenerator(data_folder=TRAIN_VIDEO_FOLDER, labels_df=train_labels_df, label_encoder=label_encoder, batch_size=BATCH_SIZE, num_classes=NUM_CLASSES)\n",
        "validation_generator = SignLanguageGenerator(data_folder=VAL_VIDEO_FOLDER, labels_df=val_labels_df, label_encoder=label_encoder, batch_size=BATCH_SIZE, num_classes=NUM_CLASSES)\n",
        "print(\"\\nData Generators are ready for optimization trials.\")\n",
        "# ...\n",
        "def objective(trial):\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True); lstm_units = trial.suggest_int('lstm_units', 64, 256); dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)\n",
        "    base_model = MobileNetV2(input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights='imagenet'); base_model.trainable = False\n",
        "    video_input = Input(shape=(MAX_FRAMES, IMG_SIZE, IMG_SIZE, 3)); cnn_features = TimeDistributed(base_model)(video_input)\n",
        "    cnn_features = TimeDistributed(GlobalAveragePooling2D())(cnn_features); cnn_features = Dropout(dropout_rate)(cnn_features)\n",
        "    lstm_output = LSTM(lstm_units)(cnn_features); lstm_output = Dropout(dropout_rate)(lstm_output)\n",
        "    output_layer = Dense(NUM_CLASSES, activation='softmax')(lstm_output); model = Model(inputs=video_input, outputs=output_layer)\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=0)\n",
        "    history = model.fit(train_generator, validation_data=validation_generator, epochs=20, callbacks=[early_stopping], verbose=0)\n",
        "    val_accuracy = np.max(history.history['val_accuracy']); return val_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4raXj7tnIXLK",
        "outputId": "52f8d16c-f57b-4141-b7dc-4ddbde86f49a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator for 'frontal_train_videos/raw_videos' initialized with 31047 valid files.\n",
            "Generator for 'frontal_val_videos/raw_videos' initialized with 1739 valid files.\n",
            "\n",
            "Data Generators are ready for optimization trials.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "# --- 1. 创建一个Optuna研究 ---\n",
        "# direction='maximize' 表示我们希望目标函数(objective)的返回值越大越好\n",
        "study = optuna.create_study(direction='maximize')\n",
        "\n",
        "# --- 2. 开始优化 ---\n",
        "# n_trials=25 表示我们将进行25次不同的超参数组合试验\n",
        "print(\"Starting Bayesian Optimization for 25 trials...\")\n",
        "study.optimize(objective, n_trials=25)\n",
        "\n",
        "# --- 3. 打印最佳结果 ---\n",
        "print(\"\\n--- Optimization Finished ---\")\n",
        "print(f\"Best trial number: {study.best_trial.number}\")\n",
        "print(f\"Best validation accuracy: {study.best_value:.4f}\")\n",
        "print(\"Best hyperparameters found:\")\n",
        "for key, value in study.best_params.items():\n",
        "    print(f\"  - {key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqR1ITViIY36",
        "outputId": "d04c0f24-f68d-4015-eb21-fa14c5ef0066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-07-18 20:22:41,252] A new study created in memory with name: no-name-eee6c93c-7e38-4cd6-a8cc-546cd0a16703\n",
            "/tmp/ipython-input-7-3913354600.py:97: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights='imagenet'); base_model.trainable = False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Bayesian Optimization for 25 trials...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n",
            "[I 2025-07-19 03:07:12,173] Trial 0 finished with value: 0.0017361111240461469 and parameters: {'learning_rate': 0.00021001054934091255, 'lstm_units': 93, 'dropout_rate': 0.45079469677480605}. Best is trial 0 with value: 0.0017361111240461469.\n",
            "/tmp/ipython-input-7-3913354600.py:97: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights='imagenet'); base_model.trainable = False\n"
          ]
        }
      ]
    }
  ]
}