{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq8kDi0PVoln",
        "outputId": "5eb2e753-1c7c-485b-ce9d-b87dd54b8cf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading file from Google Drive link...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1qTIXFsu8M55HrCiaGv7vZ7GkdB3ubjaG\n",
            "From (redirected): https://drive.google.com/uc?id=1qTIXFsu8M55HrCiaGv7vZ7GkdB3ubjaG&confirm=t&uuid=7fec0082-fcea-42b6-ac49-b838732148b1\n",
            "To: /content/rgb_videos.zip\n",
            "100%|██████████| 2.41G/2.41G [00:31<00:00, 76.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete.\n",
            "\n",
            "Extracting rgb_videos.zip to dev_test_data_rgb...\n",
            "Extraction complete.\n",
            "\n",
            "--- Verifying Extracted Videos ---\n",
            "Found 2343 video files in 'dev_test_data_rgb/raw_videos'.\n",
            "Here are the first few:\n",
            "  - G3g0-BeFN3c_29-5-rgb_front.mp4\n",
            "  - G3bMqicS4bQ_16-5-rgb_front.mp4\n",
            "  - g1xdqxCZxTg_5-3-rgb_front.mp4\n",
            "  - g1ccEYTMGGY_16-10-rgb_front.mp4\n",
            "  - G25fic3QxDk_1-10-rgb_front.mp4\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# --- Step 1: Download the file from the Google Drive link ---\n",
        "# [cite_start]This link corresponds to the \"Green Screen RGB clips* (TEST)\" data. [cite: 13, 14]\n",
        "file_id = '1qTIXFsu8M55HrCiaGv7vZ7GkdB3ubjaG'\n",
        "download_path = 'rgb_videos.zip'\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "print(f\"Downloading file from Google Drive link...\")\n",
        "gdown.download(url, download_path, quiet=False)\n",
        "print(\"Download complete.\")\n",
        "\n",
        "# --- Step 2: Unzip the downloaded file ---\n",
        "extract_path = 'dev_test_data_rgb'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "if not os.path.exists(extract_path):\n",
        "    os.makedirs(extract_path)\n",
        "\n",
        "print(f\"\\nExtracting {download_path} to {extract_path}...\")\n",
        "with zipfile.ZipFile(download_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "print(\"Extraction complete.\")\n",
        "\n",
        "# --- Step 3: Verify Contents ---\n",
        "print(\"\\n--- Verifying Extracted Videos ---\")\n",
        "# The zip file contains a nested folder, so we need to look inside it.\n",
        "# e.g., Green_Screen_RGB_clips_TEST/\n",
        "extracted_folder_name = os.listdir(extract_path)[0] # Get the name of the single folder inside\n",
        "video_folder_path = os.path.join(extract_path, extracted_folder_name)\n",
        "\n",
        "if os.path.isdir(video_folder_path):\n",
        "    video_files = [f for f in os.listdir(video_folder_path) if f.endswith('.mp4')]\n",
        "    print(f\"Found {len(video_files)} video files in '{video_folder_path}'.\")\n",
        "    print(\"Here are the first few:\")\n",
        "    for video_name in video_files[:5]:\n",
        "        print(f\"  - {video_name}\")\n",
        "else:\n",
        "    print(\"Could not find the nested video folder. Please check the contents of the zip file.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import pandas as pd\n",
        "\n",
        "# --- Step 1: Download the manually re-aligned CSV from Google Drive ---\n",
        "\n",
        "# IMPORTANT: Replace 'YOUR_FILE_ID_HERE' with the actual file ID from the shareable link.\n",
        "file_id = '1AgwBZW26kFHS4CWNMQTCMPGkBPkH3qCu'\n",
        "output_path = 'manual_realigned_labels.csv'\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "print(f\"Downloading the re-aligned labels CSV...\")\n",
        "gdown.download(url, output_path, quiet=False)\n",
        "print(\"Download complete.\")\n",
        "\n",
        "# --- Step 2: Load the CSV into a pandas DataFrame and display the first few rows ---\n",
        "try:\n",
        "    labels_df = pd.read_csv(output_path)\n",
        "    print(\"\\nSuccessfully loaded the CSV into a DataFrame. Here are the first 5 rows:\")\n",
        "    print(labels_df.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\nERROR: Could not find the downloaded file at '{output_path}'. The download may have failed.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the CSV: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HTfQDVcYkgd",
        "outputId": "8f1a7354-4aa8-4bb5-e2d9-83c6a9815b2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading the re-aligned labels CSV...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1AgwBZW26kFHS4CWNMQTCMPGkBPkH3qCu\n",
            "To: /content/manual_realigned_labels.csv\n",
            "100%|██████████| 424k/424k [00:00<00:00, 76.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete.\n",
            "An error occurred while reading the CSV: Error tokenizing data. C error: Expected 1 fields in line 4, saw 2\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the path to the downloaded file\n",
        "output_path = 'manual_realigned_labels.csv'\n",
        "\n",
        "try:\n",
        "    # --- The Correct Fix ---\n",
        "    # We now use sep='\\t' to specify that the columns are separated by tabs.\n",
        "    labels_df = pd.read_csv(output_path, sep='\\t')\n",
        "\n",
        "    print(\"Successfully loaded the CSV using a tab delimiter.\")\n",
        "    print(\"Here is the DataFrame head:\")\n",
        "    print(labels_df.head())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNXrUq8CZHEU",
        "outputId": "f74c358c-1111-4b7c-d24f-2111c580ef97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded the CSV using a tab delimiter.\n",
            "Here is the DataFrame head:\n",
            "      VIDEO_ID               VIDEO_NAME    SENTENCE_ID  \\\n",
            "0  -fZc293MpJk  -fZc293MpJk-1-rgb_front  -fZc293MpJk_0   \n",
            "1  -fZc293MpJk  -fZc293MpJk-1-rgb_front  -fZc293MpJk_2   \n",
            "2  -fZc293MpJk  -fZc293MpJk-1-rgb_front  -fZc293MpJk_3   \n",
            "3  -fZc293MpJk  -fZc293MpJk-1-rgb_front  -fZc293MpJk_4   \n",
            "4  -fZc293MpJk  -fZc293MpJk-1-rgb_front  -fZc293MpJk_5   \n",
            "\n",
            "               SENTENCE_NAME  START_REALIGNED  END_REALIGNED  \\\n",
            "0  -fZc293MpJk_0-1-rgb_front             0.26           6.79   \n",
            "1  -fZc293MpJk_2-1-rgb_front             7.27          20.30   \n",
            "2  -fZc293MpJk_3-1-rgb_front            21.25          25.51   \n",
            "3  -fZc293MpJk_4-1-rgb_front            27.75          44.64   \n",
            "4  -fZc293MpJk_5-1-rgb_front            46.68          52.44   \n",
            "\n",
            "                                            SENTENCE  \n",
            "0                                                Hi!  \n",
            "1  The aileron is the control surface in the wing...  \n",
            "2  By moving the stick, you cause pressure to inc...  \n",
            "3  The elevator is the part that moves with the s...  \n",
            "4  Therefore, it's either going uphill, downhill,...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd # Assuming labels_df is in memory from the previous step\n",
        "\n",
        "# Define image dimensions and the path to the video folder\n",
        "IMG_SIZE = 64\n",
        "VIDEO_FOLDER_PATH = 'dev_test_data_rgb/raw_videos'\n",
        "\n",
        "def load_video(video_path):\n",
        "    \"\"\"\n",
        "    Loads a video file, extracts its frames, resizes them, and returns\n",
        "    them as a NumPy array.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        # Resize the frame to our standard size and normalize pixel values\n",
        "        resized_frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
        "        # You could also normalize here by dividing by 255.0\n",
        "        frames.append(resized_frame)\n",
        "    cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "# --- Test the function on a single video ---\n",
        "\n",
        "# Let's grab the first video from our DataFrame to test\n",
        "if 'labels_df' in locals() and not labels_df.empty:\n",
        "    sample_video_name = labels_df.loc[0, 'SENTENCE_NAME'] + '.mp4'\n",
        "    sample_video_path = os.path.join(VIDEO_FOLDER_PATH, sample_video_name)\n",
        "\n",
        "    print(f\"Loading sample video: {sample_video_path}\")\n",
        "\n",
        "    if os.path.exists(sample_video_path):\n",
        "        # Load the video frames\n",
        "        video_frames = load_video(sample_video_path)\n",
        "\n",
        "        # Display the output shape\n",
        "        # Shape will be (num_frames, height, width, color_channels)\n",
        "        print(\"\\n--- Video Preprocessing Test Complete ---\")\n",
        "        print(f\"Shape of the processed video data: {video_frames.shape}\")\n",
        "        print(f\"Number of frames extracted: {video_frames.shape[0]}\")\n",
        "        print(f\"Frame dimensions (Height, Width): {video_frames.shape[1]}x{video_frames.shape[2]}\")\n",
        "        print(f\"Color channels (BGR): {video_frames.shape[3]}\")\n",
        "    else:\n",
        "        print(f\"ERROR: Sample video not found at '{sample_video_path}'\")\n",
        "        print(\"Please ensure the VIDEO_FOLDER_PATH is correct and the video files are in it.\")\n",
        "else:\n",
        "    print(\"ERROR: `labels_df` not found or is empty. Please run the previous cell to load the labels CSV.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6XcPRRgaFj1",
        "outputId": "e2d53555-8864-4072-ad8c-3e4a33f6cc63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading sample video: dev_test_data_rgb/raw_videos/-fZc293MpJk_0-1-rgb_front.mp4\n",
            "\n",
            "--- Video Preprocessing Test Complete ---\n",
            "Shape of the processed video data: (17, 64, 64, 3)\n",
            "Number of frames extracted: 17\n",
            "Frame dimensions (Height, Width): 64x64\n",
            "Color channels (BGR): 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# --- Parameters ---\n",
        "# We'll process a smaller subset for this smoke test to save time.\n",
        "NUM_SAMPLES_TO_PROCESS = 50\n",
        "MAX_FRAMES = 30 # As per the plan, we need a fixed number of frames for the model.\n",
        "IMG_SIZE = 64\n",
        "VIDEO_FOLDER_PATH = 'dev_test_data_rgb/raw_videos'\n",
        "\n",
        "# --- Data Processing Loop ---\n",
        "# Ensure the DataFrame is available\n",
        "if 'labels_df' not in locals() or labels_df.empty:\n",
        "    print(\"ERROR: `labels_df` not found. Please run the cell that loads the CSV.\")\n",
        "else:\n",
        "    X_frames = []\n",
        "    # Take a subset of the data for this test\n",
        "    data_subset = labels_df.head(NUM_SAMPLES_TO_PROCESS)\n",
        "\n",
        "    for index, row in data_subset.iterrows():\n",
        "        video_name = row['SENTENCE_NAME'] + '.mp4'\n",
        "        video_path = os.path.join(VIDEO_FOLDER_PATH, video_name)\n",
        "\n",
        "        if os.path.exists(video_path):\n",
        "            frames = load_video(video_path)\n",
        "\n",
        "            # Standardize the number of frames\n",
        "            if len(frames) > MAX_FRAMES:\n",
        "                # Truncate if too long\n",
        "                frames = frames[:MAX_FRAMES]\n",
        "            elif len(frames) < MAX_FRAMES:\n",
        "                # Pad with black frames if too short\n",
        "                padding_needed = MAX_FRAMES - len(frames)\n",
        "                pad_width = ((0, padding_needed), (0, 0), (0, 0), (0, 0))\n",
        "                frames = np.pad(frames, pad_width, mode='constant', constant_values=0)\n",
        "\n",
        "            X_frames.append(frames)\n",
        "            print(f\"Processed {video_name}, shape: {frames.shape}\")\n",
        "\n",
        "    # Convert the list of video arrays into a single large array\n",
        "    X_frames = np.array(X_frames)\n",
        "\n",
        "    print(\"\\n--- Full Video Preprocessing Complete ---\")\n",
        "    print(f\"Final shape of X_frames: {X_frames.shape}\")\n",
        "    print(f\"This shape represents (num_samples, num_frames, height, width, channels).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLX7hmBlaTLQ",
        "outputId": "eac7925c-852b-485d-f6cb-af6158fb2504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed -fZc293MpJk_0-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -fZc293MpJk_2-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -fZc293MpJk_3-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -fZc293MpJk_4-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -fZc293MpJk_5-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -fZc293MpJk_6-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -fZc293MpJk_7-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0iPSnQt6w_0-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0iPSnQt6w_1-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0iPSnQt6w_10-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0iPSnQt6w_11-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0iPSnQt6w_12-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0iPSnQt6w_13-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0iPSnQt6w_14-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0iPSnQt6w_15-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0iPSnQt6w_16-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0iPSnQt6w_2-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0iPSnQt6w_3-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0iPSnQt6w_4-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0iPSnQt6w_5-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0iPSnQt6w_6-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0iPSnQt6w_7-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0iPSnQt6w_8-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0iPSnQt6w_9-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0sqksgyc4_0-2-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0sqksgyc4_1-2-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0sqksgyc4_10-2-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0sqksgyc4_2-2-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0sqksgyc4_3-2-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0sqksgyc4_4-2-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0sqksgyc4_5-2-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0sqksgyc4_6-2-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0sqksgyc4_7-2-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0sqksgyc4_8-2-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g0sqksgyc4_9-2-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g45vqccdzI_1-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g45vqccdzI_10-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g45vqccdzI_2-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g45vqccdzI_3-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g45vqccdzI_4-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g45vqccdzI_5-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g45vqccdzI_6-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g45vqccdzI_7-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g45vqccdzI_8-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed -g45vqccdzI_9-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed 37ZtKNf6Yd8_0-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed 37ZtKNf6Yd8_1-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed 37ZtKNf6Yd8_2-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed 37ZtKNf6Yd8_3-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "Processed 37ZtKNf6Yd8_4-1-rgb_front.mp4, shape: (30, 64, 64, 3)\n",
            "\n",
            "--- Full Video Preprocessing Complete ---\n",
            "Final shape of X_frames: (50, 30, 64, 64, 3)\n",
            "This shape represents (num_samples, num_frames, height, width, channels).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, TimeDistributed, LSTM, Dense, GlobalAveragePooling2D\n",
        "\n",
        "# Define the model input shape\n",
        "# (num_frames, height, width, channels)\n",
        "input_shape = (30, 64, 64, 3)\n",
        "\n",
        "# Load a pre-trained CNN model (MobileNetV2) as the feature extractor\n",
        "# We exclude the top classification layer and freeze the weights.\n",
        "base_model = MobileNetV2(\n",
        "    input_shape=(64, 64, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "base_model.trainable = False\n",
        "\n",
        "# --- Build the full CNN-LSTM architecture ---\n",
        "# Define the input layer that expects sequences of video frames\n",
        "video_input = Input(shape=input_shape)\n",
        "\n",
        "# Use TimeDistributed to apply the CNN to each frame of the video\n",
        "# This creates our feature extractor\n",
        "cnn_features = TimeDistributed(base_model)(video_input)\n",
        "cnn_features = TimeDistributed(GlobalAveragePooling2D())(cnn_features) # Flatten the features for each frame\n",
        "\n",
        "# Feed the sequence of features into the LSTM layer to understand the motion\n",
        "lstm_output = LSTM(64)(cnn_features)\n",
        "\n",
        "# Add a Dense layer for classification\n",
        "# We'll use 10 output classes as a placeholder for the smoke test\n",
        "num_classes = 10\n",
        "output_layer = Dense(num_classes, activation='softmax')(lstm_output)\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs=video_input, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Print the model summary to see the architecture\n",
        "print(\"--- CNN-LSTM Model Architecture ---\")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "jthNAKWga0th",
        "outputId": "d2f45b68-6b77-4a51-ddcb-928db6203370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-9-1059992731.py:11: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "--- CNN-LSTM Model Architecture ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m1280\u001b[0m) │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1280\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │       \u001b[38;5;34m344,320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">344,320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,602,954\u001b[0m (9.93 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,602,954</span> (9.93 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m344,970\u001b[0m (1.32 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">344,970</span> (1.32 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, TimeDistributed, LSTM, Dense, GlobalAveragePooling2D\n",
        "\n",
        "# --- Step 1: Prepare Labels and Get Class Count ---\n",
        "data_subset = labels_df.head(50)\n",
        "lb = LabelBinarizer()\n",
        "y_labels = lb.fit_transform(data_subset['SENTENCE'])\n",
        "num_classes = y_labels.shape[1] # Dynamically get the number of classes\n",
        "\n",
        "print(f\"Dynamically determined number of classes: {num_classes}\")\n",
        "\n",
        "# --- Step 2: Re-build the Model with the Correct Output Layer ---\n",
        "input_shape = (30, 64, 64, 3)\n",
        "base_model = MobileNetV2(input_shape=(64, 64, 3), include_top=False, weights='imagenet')\n",
        "base_model.trainable = False\n",
        "\n",
        "video_input = Input(shape=input_shape)\n",
        "cnn_features = TimeDistributed(base_model)(video_input)\n",
        "cnn_features = TimeDistributed(GlobalAveragePooling2D())(cnn_features)\n",
        "lstm_output = LSTM(64)(cnn_features)\n",
        "\n",
        "# Use the correct number of classes in the final layer\n",
        "output_layer = Dense(num_classes, activation='softmax')(lstm_output)\n",
        "\n",
        "model = Model(inputs=video_input, outputs=output_layer)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"\\n--- Model Re-built with Correct Output Shape ---\")\n",
        "model.summary()\n",
        "\n",
        "# --- Step 3: Split Data and Train the Corrected Model ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_frames, y_labels, test_size=0.20, random_state=42)\n",
        "\n",
        "print(\"\\n--- Starting Model Training ---\")\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=4,\n",
        "    batch_size=8,\n",
        "    validation_data=(X_test, y_test)\n",
        ")\n",
        "\n",
        "print(\"\\n--- Smoke Test Complete ---\")\n",
        "print(\"The CNN-LSTM pipeline ran successfully from start to finish.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "vnn0NGwybHzB",
        "outputId": "3e1f270b-d724-4c20-920e-81e075b4d5d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dynamically determined number of classes: 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-11-3833964658.py:19: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(input_shape=(64, 64, 3), include_top=False, weights='imagenet')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model Re-built with Correct Output Shape ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m1280\u001b[0m) │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_3              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1280\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │       \u001b[38;5;34m344,320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │         \u001b[38;5;34m3,250\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_3              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">344,320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,250</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,605,554\u001b[0m (9.94 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,605,554</span> (9.94 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m347,570\u001b[0m (1.33 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">347,570</span> (1.33 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Model Training ---\n",
            "Epoch 1/4\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 15s/step - accuracy: 0.0000e+00 - loss: 4.1349 - val_accuracy: 0.0000e+00 - val_loss: 4.2985\n",
            "Epoch 2/4\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 947ms/step - accuracy: 0.1240 - loss: 3.6921 - val_accuracy: 0.0000e+00 - val_loss: 4.4558\n",
            "Epoch 3/4\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.1510 - loss: 3.5444 - val_accuracy: 0.0000e+00 - val_loss: 4.5125\n",
            "Epoch 4/4\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.1365 - loss: 3.3192 - val_accuracy: 0.0000e+00 - val_loss: 4.8045\n",
            "\n",
            "--- Smoke Test Complete ---\n",
            "The CNN-LSTM pipeline ran successfully from start to finish.\n"
          ]
        }
      ]
    }
  ]
}