{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQCmWnoe03sA",
        "outputId": "d6c0e828-31fa-4fa5-f140-a4f815439cd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully.\n",
            "\n",
            "Copying zip files from Drive to local environment...\n",
            "Zip files copied.\n",
            "\n",
            "Extracting train videos locally...\n",
            "Train videos extracted.\n",
            "\n",
            "Extracting validation videos locally...\n",
            "Validation videos extracted.\n",
            "\n",
            "Local zip files cleaned up.\n",
            "--- Data is now ready on the fast local disk ---\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# --- 1. Mount Drive ---\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted successfully.\")\n",
        "\n",
        "# --- 2. Define the route ---\n",
        "base_drive_path = '/content/drive/MyDrive/train-CNN+LSTM+BO'\n",
        "train_zip_gdrive_path = os.path.join(base_drive_path, 'train_rgb_front_clips.zip')\n",
        "val_zip_gdrive_path = os.path.join(base_drive_path, 'val_rgb_front_clips.zip')\n",
        "\n",
        "# Colab_local_route\n",
        "train_zip_local_path = 'train_rgb_front_clips.zip'\n",
        "val_zip_local_path = 'val_rgb_front_clips.zip'\n",
        "train_extract_folder = 'frontal_train_videos'\n",
        "val_extract_folder = 'frontal_val_videos'\n",
        "\n",
        "# --- 3. Fetch .zip from drive ---\n",
        "print(\"\\nCopying zip files from Drive to local environment...\")\n",
        "shutil.copy(train_zip_gdrive_path, train_zip_local_path)\n",
        "shutil.copy(val_zip_gdrive_path, val_zip_local_path)\n",
        "print(\"Zip files copied.\")\n",
        "\n",
        "# --- 4. Unzip the .zip ---\n",
        "print(\"\\nExtracting train videos locally...\")\n",
        "os.makedirs(train_extract_folder, exist_ok=True)\n",
        "with zipfile.ZipFile(train_zip_local_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(train_extract_folder)\n",
        "print(\"Train videos extracted.\")\n",
        "\n",
        "print(\"\\nExtracting validation videos locally...\")\n",
        "os.makedirs(val_extract_folder, exist_ok=True)\n",
        "with zipfile.ZipFile(val_zip_local_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(val_extract_folder)\n",
        "print(\"Validation videos extracted.\")\n",
        "\n",
        "# --- 5. Clear the .zip to save storage ---\n",
        "os.remove(train_zip_local_path)\n",
        "os.remove(val_zip_local_path)\n",
        "print(\"\\nLocal zip files cleaned up.\")\n",
        "print(\"--- Data is now ready on the fast local disk ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YNsXeFB8iEwI",
        "outputId": "a898c553-5085-4ffe-af6c-90815431a437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n",
            "\n",
            "Copying label files from Google Drive to local environment...\n",
            "Label files are ready in the local environment.\n",
            "\n",
            "Total unique classes found: 31592\n",
            "\n",
            "Creating Data Generators...\n",
            "Verifying files for generator in 'frontal_train_videos/raw_videos'...\n",
            "Found 31047 valid and labeled video files.\n",
            "Verifying files for generator in 'frontal_val_videos/raw_videos'...\n",
            "Found 1739 valid and labeled video files.\n",
            "Data Generators are ready.\n",
            "\n",
            "Found existing model at '/content/drive/MyDrive/train-CNN+LSTM+BO/cnn_lstm_frontal_model_v1.h5'. Loading to resume training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded and re-compiled successfully.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">721,408</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31592</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,075,368</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m1280\u001b[0m) │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1280\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1280\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m721,408\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31592\u001b[0m)          │     \u001b[38;5;34m4,075,368\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,054,760</span> (26.91 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,054,760\u001b[0m (26.91 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,796,776</span> (18.30 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,796,776\u001b[0m (18.30 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting or Resuming CNN-LSTM Model Training ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.0019 - loss: 10.3464"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: val_accuracy improved from -inf to 0.00116, saving model to /content/drive/MyDrive/train-CNN+LSTM+BO/cnn_lstm_frontal_model_v1.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6111s\u001b[0m 6s/step - accuracy: 0.0019 - loss: 10.3465 - val_accuracy: 0.0012 - val_loss: 11.1479\n",
            "Epoch 2/50\n",
            "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.0026 - loss: 10.1945\n",
            "Epoch 2: val_accuracy did not improve from 0.00116\n",
            "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5958s\u001b[0m 6s/step - accuracy: 0.0026 - loss: 10.1945 - val_accuracy: 0.0012 - val_loss: 11.7448\n",
            "Epoch 3/50\n",
            "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.0031 - loss: 9.9439\n",
            "Epoch 3: val_accuracy did not improve from 0.00116\n",
            "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5906s\u001b[0m 6s/step - accuracy: 0.0031 - loss: 9.9439 - val_accuracy: 0.0012 - val_loss: 12.1941\n",
            "Epoch 4/50\n",
            "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.0043 - loss: 9.6834\n",
            "Epoch 4: val_accuracy did not improve from 0.00116\n",
            "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5919s\u001b[0m 6s/step - accuracy: 0.0043 - loss: 9.6833 - val_accuracy: 0.0012 - val_loss: 12.6353\n",
            "Epoch 5/50\n",
            "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.0034 - loss: 9.3710\n",
            "Epoch 5: val_accuracy did not improve from 0.00116\n",
            "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5889s\u001b[0m 6s/step - accuracy: 0.0034 - loss: 9.3709 - val_accuracy: 0.0012 - val_loss: 12.9922\n",
            "Epoch 6/50\n",
            "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.0075 - loss: 9.0020\n",
            "Epoch 6: val_accuracy did not improve from 0.00116\n",
            "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5874s\u001b[0m 6s/step - accuracy: 0.0075 - loss: 9.0020 - val_accuracy: 0.0012 - val_loss: 13.1517\n",
            "Epoch 6: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\n",
            "--- Model Training Complete ---\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# Phase 2: CNN-LSTM\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 0. IMPORT ---\n",
        "from google.colab import drive\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import math\n",
        "import shutil\n",
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, TimeDistributed, LSTM, Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# --- 1. Mount drive and prepare ---\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "print(\"Google Drive mounted successfully.\")\n",
        "\n",
        "base_drive_path = '/content/drive/MyDrive/train-CNN+LSTM+BO'\n",
        "TRAIN_LABELS_CSV_GDRIVE = os.path.join(base_drive_path, 'how2sign_realigned_train.csv')\n",
        "VAL_LABELS_CSV_GDRIVE = os.path.join(base_drive_path, 'how2sign_realigned_val.csv')\n",
        "TRAIN_LABELS_CSV_LOCAL = 'how2sign_realigned_train.csv'\n",
        "VAL_LABELS_CSV_LOCAL = 'how2sign_realigned_val.csv'\n",
        "\n",
        "print(\"\\nCopying label files from Google Drive to local environment...\")\n",
        "shutil.copy(TRAIN_LABELS_CSV_GDRIVE, TRAIN_LABELS_CSV_LOCAL)\n",
        "shutil.copy(VAL_LABELS_CSV_GDRIVE, VAL_LABELS_CSV_LOCAL)\n",
        "print(\"Label files are ready in the local environment.\")\n",
        "\n",
        "# --- 2. Define constants, load labels, and create encoder. ---\n",
        "IMG_SIZE = 64; MAX_FRAMES = 30; BATCH_SIZE = 32\n",
        "TRAIN_VIDEO_FOLDER = 'frontal_train_videos/raw_videos'; VAL_VIDEO_FOLDER = 'frontal_val_videos/raw_videos'\n",
        "train_labels_df = pd.read_csv(TRAIN_LABELS_CSV_LOCAL, sep='\\t')\n",
        "val_labels_df = pd.read_csv(VAL_LABELS_CSV_LOCAL, sep='\\t')\n",
        "all_labels_df = pd.concat([train_labels_df, val_labels_df], ignore_index=True)\n",
        "label_encoder = LabelEncoder(); label_encoder.fit(all_labels_df['SENTENCE'])\n",
        "NUM_CLASSES = len(label_encoder.classes_)\n",
        "print(f\"\\nTotal unique classes found: {NUM_CLASSES}\")\n",
        "\n",
        "# --- 3. SignLanguageGenerator ---\n",
        "class SignLanguageGenerator(Sequence):\n",
        "    def __init__(self, data_folder, labels_df, label_encoder, batch_size, num_classes):\n",
        "        self.data_folder = data_folder; self.label_encoder = label_encoder; self.batch_size = batch_size; self.num_classes = num_classes\n",
        "        self.labels_df = labels_df.copy()\n",
        "        print(f\"Verifying files for generator in '{self.data_folder}'...\")\n",
        "        all_disk_files = {os.path.splitext(f)[0] for f in os.listdir(self.data_folder) if f.endswith('.mp4')}\n",
        "        all_csv_files = set(self.labels_df['SENTENCE_NAME'].tolist()); valid_files = list(all_disk_files.intersection(all_csv_files))\n",
        "        self.video_files = valid_files; self.labels_df = self.labels_df[self.labels_df['SENTENCE_NAME'].isin(self.video_files)]\n",
        "        print(f\"Found {len(self.video_files)} valid and labeled video files.\")\n",
        "    def __len__(self): return math.floor(len(self.video_files) / self.batch_size)\n",
        "    def __getitem__(self, idx):\n",
        "        batch_files = self.video_files[idx * self.batch_size:(idx + 1) * self.batch_size]; batch_labels_df = self.labels_df[self.labels_df['SENTENCE_NAME'].isin(batch_files)]\n",
        "        X = np.zeros((len(batch_files), MAX_FRAMES, IMG_SIZE, IMG_SIZE, 3), dtype=np.float32); y_text = []\n",
        "        for i, row in enumerate(batch_labels_df.itertuples()):\n",
        "            video_path = os.path.join(self.data_folder, row.SENTENCE_NAME + '.mp4'); cap = cv2.VideoCapture(video_path); frames = []\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret: break\n",
        "                resized_frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE)); frames.append(resized_frame)\n",
        "            cap.release(); frames = np.array(frames)\n",
        "            if frames.size == 0: continue\n",
        "            if len(frames) > MAX_FRAMES: frames = frames[:MAX_FRAMES]\n",
        "            elif len(frames) < MAX_FRAMES:\n",
        "                pad_width = ((0, MAX_FRAMES - len(frames)), (0, 0), (0, 0), (0, 0)); frames = np.pad(frames, pad_width, mode='constant', constant_values=0)\n",
        "            X[i,] = frames / 255.0; y_text.append(row.SENTENCE)\n",
        "        y_int = self.label_encoder.transform(y_text); y = to_categorical(y_int, num_classes=self.num_classes)\n",
        "        return X, y\n",
        "\n",
        "# --- 4. Instantiation generator ---\n",
        "print(\"\\nCreating Data Generators...\")\n",
        "train_generator = SignLanguageGenerator(data_folder=TRAIN_VIDEO_FOLDER, labels_df=train_labels_df, label_encoder=label_encoder, batch_size=BATCH_SIZE, num_classes=NUM_CLASSES)\n",
        "validation_generator = SignLanguageGenerator(data_folder=VAL_VIDEO_FOLDER, labels_df=val_labels_df, label_encoder=label_encoder, batch_size=BATCH_SIZE, num_classes=NUM_CLASSES)\n",
        "print(\"Data Generators are ready.\")\n",
        "\n",
        "# --- 5. Load or build model (with　check) ---\n",
        "model_path = '/content/drive/MyDrive/train-CNN+LSTM+BO/cnn_lstm_frontal_model_v1.h5'\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    print(f\"\\nFound existing model at '{model_path}'. Loading to resume training...\")\n",
        "    model = load_model(model_path)\n",
        "    # ecompile to ensure the state is correct, especially for options like `run_eagerly`\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    print(\"Model loaded and re-compiled successfully.\")\n",
        "else:\n",
        "    print(\"\\nNo existing model found. Building a new model from scratch...\")\n",
        "    input_shape = (MAX_FRAMES, IMG_SIZE, IMG_SIZE, 3)\n",
        "    base_model = MobileNetV2(input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights='imagenet')\n",
        "    base_model.trainable = False\n",
        "    video_input = Input(shape=input_shape)\n",
        "    cnn_features = TimeDistributed(base_model)(video_input)\n",
        "    cnn_features = TimeDistributed(GlobalAveragePooling2D())(cnn_features)\n",
        "    cnn_features = Dropout(0.5)(cnn_features)\n",
        "    lstm_output = LSTM(128)(cnn_features)\n",
        "    lstm_output = Dropout(0.5)(lstm_output)\n",
        "    output_layer = Dense(NUM_CLASSES, activation='softmax')(lstm_output)\n",
        "    model = Model(inputs=video_input, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    print(\"New model built and compiled successfully.\")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# --- 6. 定义回调函数 ---\n",
        "checkpoint_path = model_path # 路径保持一致\n",
        "model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_best_only=True, monitor='val_accuracy', mode='max', verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "\n",
        "# --- 7. 开始或继续训练 ---\n",
        "print(\"\\n--- Starting or Resuming CNN-LSTM Model Training ---\")\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=50,\n",
        "    callbacks=[model_checkpoint, early_stopping]\n",
        ")\n",
        "\n",
        "print(\"\\n--- Model Training Complete ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m59-BWtv77r_",
        "outputId": "cd4c9846-50e1-4fd3-cecc-0ae2842f18ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully.\n",
            "\n",
            "Copying label files from Google Drive to local environment for faster access...\n",
            "Label files are ready in the local environment.\n",
            "\n",
            "Total unique classes found: 31592\n",
            "\n",
            "Creating Data Generators...\n",
            "Data Generators are ready.\n",
            "\n",
            "Building CNN-LSTM model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-7-966512075.py:108: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights='imagenet')\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">721,408</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31592</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,075,368</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m1280\u001b[0m) │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1280\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1280\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m721,408\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31592\u001b[0m)          │     \u001b[38;5;34m4,075,368\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,054,760</span> (26.91 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,054,760\u001b[0m (26.91 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,796,776</span> (18.30 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,796,776\u001b[0m (18.30 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting CNN-LSTM Model Training ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 0.0017 - loss: 10.4032"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: val_accuracy improved from -inf to 0.00116, saving model to /content/drive/MyDrive/train-CNN+LSTM+BO/cnn_lstm_frontal_model_v1.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8254s\u001b[0m 8s/step - accuracy: 0.0017 - loss: 10.4033 - val_accuracy: 0.0012 - val_loss: 10.8065\n",
            "Epoch 2/50\n",
            "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.0023 - loss: 10.3423\n",
            "Epoch 2: val_accuracy did not improve from 0.00116\n",
            "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6164s\u001b[0m 6s/step - accuracy: 0.0023 - loss: 10.3422 - val_accuracy: 0.0012 - val_loss: 11.2941\n",
            "Epoch 3/50\n",
            "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.0032 - loss: 10.1566\n",
            "Epoch 3: val_accuracy did not improve from 0.00116\n",
            "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6136s\u001b[0m 6s/step - accuracy: 0.0032 - loss: 10.1566 - val_accuracy: 0.0012 - val_loss: 11.8183\n",
            "Epoch 4/50\n",
            "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.0028 - loss: 10.0513\n",
            "Epoch 4: val_accuracy did not improve from 0.00116\n",
            "\u001b[1m970/970\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6110s\u001b[0m 6s/step - accuracy: 0.0028 - loss: 10.0513 - val_accuracy: 0.0012 - val_loss: 12.3096\n",
            "Epoch 5/50\n",
            "\u001b[1m346/970\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:03:33\u001b[0m 6s/step - accuracy: 0.0026 - loss: 9.9135"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# Phase 2: CNN-LSTM 模型完整流程\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 0. 导入所有必需的库 ---\n",
        "from google.colab import drive\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import math\n",
        "import shutil\n",
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, TimeDistributed, LSTM, Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# --- 1. 挂载Drive并定义路径 ---\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted successfully.\")\n",
        "\n",
        "base_drive_path = '/content/drive/MyDrive/train-CNN+LSTM+BO'\n",
        "TRAIN_VIDEO_FOLDER = os.path.join(base_drive_path, 'extracted_frontal_videos/train/raw_videos')\n",
        "VAL_VIDEO_FOLDER = os.path.join(base_drive_path, 'extracted_frontal_videos/val/raw_videos')\n",
        "TRAIN_LABELS_CSV_GDRIVE = os.path.join(base_drive_path, 'how2sign_realigned_train.csv')\n",
        "VAL_LABELS_CSV_GDRIVE = os.path.join(base_drive_path, 'how2sign_realigned_val.csv')\n",
        "\n",
        "# 定义Colab本地路径\n",
        "TRAIN_LABELS_CSV_LOCAL = 'how2sign_realigned_train.csv'\n",
        "VAL_LABELS_CSV_LOCAL = 'how2sign_realigned_val.csv'\n",
        "\n",
        "# --- 2. 准备工作：将CSV文件从Drive复制到本地 ---\n",
        "print(\"\\nCopying label files from Google Drive to local environment for faster access...\")\n",
        "shutil.copy(TRAIN_LABELS_CSV_GDRIVE, TRAIN_LABELS_CSV_LOCAL)\n",
        "shutil.copy(VAL_LABELS_CSV_GDRIVE, VAL_LABELS_CSV_LOCAL)\n",
        "print(\"Label files are ready in the local environment.\")\n",
        "\n",
        "# --- 3. 参数与标签加载 ---\n",
        "IMG_SIZE = 64\n",
        "MAX_FRAMES = 30\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_labels_df = pd.read_csv(TRAIN_LABELS_CSV_LOCAL, sep='\\t')\n",
        "val_labels_df = pd.read_csv(VAL_LABELS_CSV_LOCAL, sep='\\t')\n",
        "all_labels_df = pd.concat([train_labels_df, val_labels_df], ignore_index=True)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(all_labels_df['SENTENCE'])\n",
        "NUM_CLASSES = len(label_encoder.classes_)\n",
        "print(f\"\\nTotal unique classes found: {NUM_CLASSES}\")\n",
        "\n",
        "# --- 4. 定义数据生成器 (SignLanguageGenerator) ---\n",
        "class SignLanguageGenerator(Sequence):\n",
        "    def __init__(self, data_folder, labels_df, label_encoder, batch_size, num_classes):\n",
        "        self.data_folder = data_folder\n",
        "        self.label_encoder = label_encoder\n",
        "        self.batch_size = batch_size\n",
        "        self.num_classes = num_classes\n",
        "        all_disk_files = {os.path.splitext(f)[0] for f in os.listdir(self.data_folder) if f.endswith('.mp4')}\n",
        "        all_csv_files = set(labels_df['SENTENCE_NAME'].tolist())\n",
        "        valid_files = list(all_disk_files.intersection(all_csv_files))\n",
        "        self.video_files = valid_files\n",
        "        self.labels_df = labels_df[labels_df['SENTENCE_NAME'].isin(self.video_files)].copy()\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.floor(len(self.video_files) / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_files = self.video_files[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_labels_df = self.labels_df[self.labels_df['SENTENCE_NAME'].isin(batch_files)]\n",
        "        X = np.zeros((len(batch_files), MAX_FRAMES, IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
        "        y_text = []\n",
        "        for i, row in enumerate(batch_labels_df.itertuples()):\n",
        "            video_path = os.path.join(self.data_folder, row.SENTENCE_NAME + '.mp4')\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "            frames = []\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret: break\n",
        "                resized_frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
        "                frames.append(resized_frame)\n",
        "            cap.release()\n",
        "            frames = np.array(frames)\n",
        "            if frames.size == 0: continue\n",
        "            if len(frames) > MAX_FRAMES:\n",
        "                frames = frames[:MAX_FRAMES]\n",
        "            elif len(frames) < MAX_FRAMES:\n",
        "                pad_width = ((0, MAX_FRAMES - len(frames)), (0, 0), (0, 0), (0, 0))\n",
        "                frames = np.pad(frames, pad_width, mode='constant', constant_values=0)\n",
        "            X[i,] = frames / 255.0\n",
        "            y_text.append(row.SENTENCE)\n",
        "        y_int = self.label_encoder.transform(y_text)\n",
        "        y = to_categorical(y_int, num_classes=self.num_classes)\n",
        "        return X, y\n",
        "\n",
        "# --- 5. 实例化生成器 ---\n",
        "print(\"\\nCreating Data Generators...\")\n",
        "train_generator = SignLanguageGenerator(data_folder=TRAIN_VIDEO_FOLDER, labels_df=train_labels_df, label_encoder=label_encoder, batch_size=BATCH_SIZE, num_classes=NUM_CLASSES)\n",
        "validation_generator = SignLanguageGenerator(data_folder=VAL_VIDEO_FOLDER, labels_df=val_labels_df, label_encoder=label_encoder, batch_size=BATCH_SIZE, num_classes=NUM_CLASSES)\n",
        "print(\"Data Generators are ready.\")\n",
        "\n",
        "# --- 6. 构建模型架构 ---\n",
        "print(\"\\nBuilding CNN-LSTM model...\")\n",
        "input_shape = (MAX_FRAMES, IMG_SIZE, IMG_SIZE, 3)\n",
        "base_model = MobileNetV2(input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights='imagenet')\n",
        "base_model.trainable = False\n",
        "video_input = Input(shape=input_shape)\n",
        "cnn_features = TimeDistributed(base_model)(video_input)\n",
        "cnn_features = TimeDistributed(GlobalAveragePooling2D())(cnn_features)\n",
        "cnn_features = Dropout(0.5)(cnn_features)\n",
        "lstm_output = LSTM(128)(cnn_features)\n",
        "lstm_output = Dropout(0.5)(lstm_output)\n",
        "output_layer = Dense(NUM_CLASSES, activation='softmax')(lstm_output)\n",
        "model = Model(inputs=video_input, outputs=output_layer)\n",
        "\n",
        "# --- 7. 编译模型 ---\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# --- 8. 定义回调函数 ---\n",
        "checkpoint_path = '/content/drive/MyDrive/train-CNN+LSTM+BO/cnn_lstm_frontal_model_v1.h5'\n",
        "model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_best_only=True, monitor='val_accuracy', mode='max', verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "\n",
        "# --- 9. 开始训练 ---\n",
        "print(\"\\n--- Starting CNN-LSTM Model Training ---\")\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=50,\n",
        "    callbacks=[model_checkpoint, early_stopping]\n",
        ")\n",
        "\n",
        "print(\"\\n--- Model Training Complete ---\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
